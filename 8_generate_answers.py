"""
CDP ë‹µë³€ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ (Diff ë°©ì‹)
- Previous CDP answers (2024ë…„ ë‹µë³€)
- CDP 2025 Updates (ì§ˆë¬¸ ë³€ê²½ì‚¬í•­)
- SK Report ì¦ê±° ê²€ìƒ‰ (RAG)
- ë¬¸ì¥ ë‹¨ìœ„ diff ìƒì„± (ìœ ì§€/ìˆ˜ì •/ì¶”ê°€/ì‚­ì œ)
"""

import json
import os
from typing import Dict, List, Any, Optional
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer, CrossEncoder
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()


class CDPAnswerGenerator:
    def __init__(self, config: Dict[str, str]):
        """
        Args:
            config: {
                "previous_answers_path": ì „ë…„ë„ CDP ë‹µë³€ JSON,
                "updates_path": CDP 2025 ì—…ë°ì´íŠ¸ JSON,
                "qdrant_path": Vector DB ê²½ë¡œ,
                "collection_name": Collection ì´ë¦„
            }
        """
        self.config = config

        # ë°ì´í„° ë¡œë“œ
        print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
        self.previous_answers = self._load_json(config["previous_answers_path"])
        self.updates = self._load_json(config["updates_path"])

        # Vector DB ì—°ê²°
        print("ğŸ”— Vector DB ì—°ê²° ì¤‘...")
        self.qdrant_client = QdrantClient(path=config["qdrant_path"])
        self.collection_name = config["collection_name"]

        # AI ëª¨ë¸ ë¡œë“œ
        print("ğŸ§  Embedding ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embedding_model = SentenceTransformer('BAAI/bge-m3')

        print("ğŸ”€ Reranking ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

        print("ğŸ¤– OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

        self.generated_answers = {}
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ\n")

    def _load_json(self, path: str) -> Dict:
        """JSON íŒŒì¼ ë¡œë“œ"""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def generate_all_answers(self, question_ids: List[str]) -> Dict[str, Any]:
        """ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""

        print("=" * 80)
        print("CDP ë‹µë³€ ì—…ë°ì´íŠ¸ ìƒì„± ì‹œì‘")
        print("=" * 80)

        for question_id in question_ids:
            print(f"\n{'=' * 80}")
            print(f"ì§ˆë¬¸ {question_id} ì²˜ë¦¬ ì¤‘...")
            print(f"{'=' * 80}")

            self.generated_answers[question_id] = self._generate_answer(question_id)

        return self.generated_answers

    def _generate_answer(self, question_id: str) -> Dict[str, Any]:
        """ê°œë³„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (Diff ë°©ì‹)"""

        # 1. Previous answer ê°€ì ¸ì˜¤ê¸°
        previous_data = self.previous_answers.get("questions", {}).get(question_id, {})
        if not previous_data:
            print(f"  âš ï¸  ì´ì „ ë‹µë³€ ì—†ìŒ")
            return {"error": "No previous answer found"}

        question_text = previous_data.get("question_text", "")
        print(f"  ğŸ“ ì§ˆë¬¸: {question_text[:80]}...")

        # 2. CDP 2025 Updates í™•ì¸
        update_info = self.updates.get("updates", {}).get(question_id)
        cdp_question_updates = self._format_cdp_updates(question_id, update_info)

        if update_info:
            print(f"  ğŸ“‹ CDP 2025 ì—…ë°ì´íŠ¸: {update_info.get('change_type', 'unknown')}")
        else:
            print(f"  ğŸ“‹ CDP 2025 ì—…ë°ì´íŠ¸: ì—†ìŒ (ë³€ê²½ì‚¬í•­ ì—†ìŒ)")

        # 3. SK Reportì—ì„œ ì¦ê±° ê²€ìƒ‰ (í•­ìƒ ì‹¤í–‰)
        print(f"  ğŸ” Vector DBì—ì„œ ì¦ê±° ê²€ìƒ‰ ì¤‘...")
        evidence = self._search_evidence(question_text)
        print(f"  âœ… {len(evidence)}ê°œ ì¦ê±° ë°œê²¬")

        # 4. LLMìœ¼ë¡œ diff ìƒì„±
        print(f"  ğŸ¤– LLMìœ¼ë¡œ ë‹µë³€ ì—…ë°ì´íŠ¸ diff ìƒì„± ì¤‘...")
        answer_updates = self._generate_diff_with_llm(
            question_id=question_id,
            question_text=question_text,
            previous_data=previous_data,
            evidence=evidence
        )

        # 5. ê²°ê³¼ êµ¬ì„±
        result = {
            "question_id": question_id,
            "question_text": question_text,
            "cdp_question_updates": cdp_question_updates,
            "previous_answer_2024": previous_data,
            "sk_2025_report_evidence": evidence[:5],  # ìƒìœ„ 5ê°œ
            "suggested_answer_updates": answer_updates,
            "review_flags": self._generate_review_flags(answer_updates, evidence)
        }

        print(f"  âœ… ì™„ë£Œ")
        return result

    def _format_cdp_updates(self, question_id: str, update_info: Optional[Dict]) -> Dict:
        """CDP ì§ˆë¬¸ ì—…ë°ì´íŠ¸ ì •ë³´ í¬ë§·"""

        if not update_info:
            return {
                "has_changes": False,
                "change_type": None,
                "description": "No changes for this question in 2025",
                "scoring_changes": None,
                "impact_on_answer": "You can use the same structure as last year",
                "source": "Corporate_Questionnaires_and_Scoring_Methodologies_Updates_2025_V1.3 (question not listed)"
            }

        change_type = update_info.get("change_type", "unknown")

        # impact ë©”ì‹œì§€ ìƒì„±
        impact_messages = {
            "correction": "âš ï¸ Scoring has changed. Review answer strategy needed.",
            "clarification": "Scoring method unchanged, but description clarified. No structural changes needed.",
            "no_change": "No changes. Same structure as last year.",
            "unknown": "Changes detected. Please review carefully."
        }

        return {
            "has_changes": True,
            "change_type": change_type,
            "description": update_info.get("description", ""),
            "scoring_changes": update_info.get("changes_to_scoring", ""),
            "impact_on_answer": impact_messages.get(change_type, impact_messages["unknown"]),
            "source": f"Corporate_Questionnaires_and_Scoring_Methodologies_Updates_2025_V1.3"
        }

    def _search_evidence(self, question_text: str, top_k: int = 5, initial_k: int = 20) -> List[Dict]:
        """Vector DBì—ì„œ ê´€ë ¨ ì¦ê±° ê²€ìƒ‰ (2ë‹¨ê³„: Vector Search + Reranking)"""

        # 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰
        query_vector = self.embedding_model.encode(question_text).tolist()

        try:
            search_results = self.qdrant_client.query_points(
                collection_name=self.collection_name,
                query=query_vector,
                limit=initial_k
            ).points

            if not search_results:
                return []

            # 2ë‹¨ê³„: Reranking
            candidates = []
            for result in search_results:
                payload = result.payload if hasattr(result, 'payload') else {}
                text = payload.get("text", payload.get("content", ""))

                candidates.append({
                    "text": text,
                    "page": payload.get("page", payload.get("page_num", 0)),
                    "vector_score": round(result.score, 3),
                    "source": "SK Inc. 2025 Sustainability Report"
                })

            # Cross-encoderë¡œ ì¬ì ìˆ˜í™”
            pairs = [[question_text, cand["text"]] for cand in candidates]
            rerank_scores = self.reranker.predict(pairs)

            for i, cand in enumerate(candidates):
                cand["rerank_score"] = round(float(rerank_scores[i]), 3)
                cand["confidence"] = cand["rerank_score"]

            # ì •ë ¬ í›„ ìƒìœ„ top_kê°œ ë°˜í™˜
            reranked = sorted(candidates, key=lambda x: x["rerank_score"], reverse=True)[:top_k]
            return reranked

        except Exception as e:
            print(f"  âš ï¸  ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _generate_diff_with_llm(
        self,
        question_id: str,
        question_text: str,
        previous_data: Dict,
        evidence: List[Dict]
    ) -> Dict:
        """LLMìœ¼ë¡œ ë¬¸ì¥ ë‹¨ìœ„ diff ìƒì„±"""

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_diff_prompt(
            question_id, question_text, previous_data, evidence
        )

        try:
            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {
                        "role": "system",
                        "content": """You are an expert CDP (Carbon Disclosure Project) consultant helping SK Inc. update their CDP 2025 submission.

Your task is to analyze previous year's answer and 2025 evidence, then generate sentence-level change suggestions.

CRITICAL OUTPUT FORMAT:
Return a JSON object with this structure:
{
  "changes": [
    {
      "type": "keep",
      "text": "sentence to keep unchanged",
      "reason": "why this is still valid (with evidence page reference)"
    },
    {
      "type": "modify",
      "old_text": "old sentence from 2024",
      "new_text": "updated sentence for 2025",
      "reason": "why this needs to change",
      "evidence_page": 45,
      "evidence_snippet": "relevant quote from 2025 report"
    },
    {
      "type": "add",
      "new_text": "entirely new sentence to add",
      "reason": "why this should be added",
      "evidence_page": 63,
      "evidence_snippet": "supporting quote"
    },
    {
      "type": "delete",
      "old_text": "sentence to remove",
      "reason": "why this is no longer relevant"
    }
  ],
  "final_suggested_answer": {
    // Same JSON structure as previous_answer, but with updated content
  }
}

IMPORTANT:
1. Analyze each sentence/field in the previous answer
2. Compare with 2025 evidence
3. Identify what should be kept, modified, added, or deleted
4. Always cite evidence page numbers
5. Keep the exact same JSON structure as previous answer"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=4000
            )

            generated_text = response.choices[0].message.content

            # JSON íŒŒì‹±
            try:
                if "```json" in generated_text:
                    generated_text = generated_text.split("```json")[1].split("```")[0].strip()
                elif "```" in generated_text:
                    generated_text = generated_text.split("```")[1].split("```")[0].strip()

                diff_result = json.loads(generated_text)
                return diff_result

            except Exception as parse_error:
                print(f"  âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨: {parse_error}")
                return {
                    "changes": [],
                    "final_suggested_answer": previous_data,
                    "parse_error": str(parse_error),
                    "raw_response": generated_text
                }

        except Exception as e:
            print(f"  âš ï¸  LLM ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                "changes": [],
                "final_suggested_answer": previous_data,
                "error": str(e)
            }

    def _build_diff_prompt(
        self,
        question_id: str,
        question_text: str,
        previous_data: Dict,
        evidence: List[Dict]
    ) -> str:
        """LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Diff ìƒì„±ìš©)"""

        # ì¦ê±° í…ìŠ¤íŠ¸ í¬ë§·
        evidence_text = "\n\n".join([
            f"Evidence {i+1} (Confidence: {ev['confidence']}, Page: {ev['page']}):\n{ev['text']}"
            for i, ev in enumerate(evidence[:5])
        ]) if evidence else "No evidence found"

        prompt = f"""
# CDP Question {question_id}

## Question Text:
{question_text}

## Previous Year's Answer (2024):
{json.dumps(previous_data, indent=2, ensure_ascii=False)}

## Evidence from SK Inc. 2025 Sustainability Report:
{evidence_text}

## Task:
Compare the 2024 answer with 2025 evidence and generate sentence-level change suggestions.

For each sentence or field in the previous answer:
1. If still valid based on 2025 evidence â†’ type: "keep"
2. If numbers/dates/facts changed â†’ type: "modify" (provide old and new text)
3. If new information should be added â†’ type: "add"
4. If no longer relevant â†’ type: "delete"

**CRITICAL REQUIREMENTS:**
1. Maintain the EXACT same JSON structure as previous answer
2. Cite evidence page numbers for all changes
3. Include evidence snippets for modifications and additions
4. Be specific about what changed (e.g., "10 facilities" â†’ "15 facilities")
5. Keep the same field_name values

**OUTPUT FORMAT:**
Return ONLY valid JSON with "changes" array and "final_suggested_answer" object.
Do NOT include explanations outside the JSON.
"""

        return prompt

    def _generate_review_flags(self, answer_updates: Dict, evidence: List[Dict]) -> Dict:
        """ê²€í†  í•„ìš” í•­ëª© í”Œë˜ê·¸ ìƒì„±"""

        reasons = []

        # ë³€ê²½ì‚¬í•­ ë¶„ì„
        changes = answer_updates.get("changes", [])

        modify_count = len([c for c in changes if c.get("type") == "modify"])
        add_count = len([c for c in changes if c.get("type") == "add"])
        delete_count = len([c for c in changes if c.get("type") == "delete"])

        if modify_count > 0:
            reasons.append(f"Content modifications ({modify_count} changes)")
        if add_count > 0:
            reasons.append(f"New content added ({add_count} additions)")
        if delete_count > 0:
            reasons.append(f"Content removed ({delete_count} deletions)")
        if len(evidence) < 2:
            reasons.append("Low evidence count (manual verification needed)")

        # Confidence íŒë‹¨
        if len(evidence) >= 3 and modify_count + add_count + delete_count <= 2:
            confidence = "high"
        elif len(evidence) >= 2:
            confidence = "medium"
        else:
            confidence = "low"

        return {
            "needs_review": len(reasons) > 0,
            "reasons": reasons,
            "confidence": confidence,
            "change_summary": {
                "modifications": modify_count,
                "additions": add_count,
                "deletions": delete_count,
                "total_changes": modify_count + add_count + delete_count
            }
        }

    def save_results(self, output_path: str):
        """ê²°ê³¼ ì €ì¥ (ì˜ë¬¸ ë²„ì „)"""

        result = {
            "metadata": {
                "year": 2025,
                "company": "SK Inc.",
                "baseline": "2024 CDP Submission",
                "cdp_updates_version": "1.3",
                "evidence_source": "SK Inc. 2025 Sustainability Report",
                "language": "en",
                "output_format": "diff"
            },
            "questions": self.generated_answers
        }

        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\n{'=' * 80}")
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"ğŸ“Š ì´ {len(self.generated_answers)}ê°œ ì§ˆë¬¸ ì²˜ë¦¬")

        # í†µê³„ ì¶œë ¥
        total_changes = 0
        for q_id, answer in self.generated_answers.items():
            changes = answer.get("suggested_answer_updates", {}).get("changes", [])
            total_changes += len(changes)

        print(f"ğŸ”„ ì´ {total_changes}ê°œ ë³€ê²½ì‚¬í•­ ì œì•ˆ")
        print(f"{'=' * 80}")


if __name__ == "__main__":
    # ì„¤ì •
    config = {
        "previous_answers_path": "config/previous_cdp_answers.json",
        "updates_path": "config/cdp_2025_updates.json",
        "qdrant_path": "data/qdrant_db",
        "collection_name": "company_docs"
    }

    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = CDPAnswerGenerator(config)

    # ì§ˆë¬¸ ì²˜ë¦¬ (6ê°œ)
    test_questions = ["2.2", "2.2.1", "2.2.2", "2.2.7", "2.3", "2.4"]

    # ë‹µë³€ ìƒì„± (ì˜ë¬¸)
    results = generator.generate_all_answers(test_questions)

    # ê²°ê³¼ ì €ì¥ (ì˜ë¬¸)
    generator.save_results("output/generated_cdp_answers_en.json")
