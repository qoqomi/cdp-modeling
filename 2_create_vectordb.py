#!/usr/bin/env python3
"""
STEP 2: ë²¡í„° DB ìƒì„±
ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•˜ê³  ì„ë² ë”©í•˜ì—¬ Qdrantì— ì €ì¥
"""

import json
import os
from pathlib import Path
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from tqdm import tqdm
from dotenv import load_dotenv

load_dotenv()

class VectorDBBuilder:
    def __init__(self):
        self.embedder = SentenceTransformer(os.getenv('EMBEDDING_MODEL', 'BAAI/bge-m3'))
        self.qdrant = QdrantClient(path=os.getenv('QDRANT_PATH', './data/qdrant_db'))
        self.collection_name = "company_docs"
        
    def init_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        try:
            self.qdrant.delete_collection(self.collection_name)
        except:
            pass
        
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
        self.qdrant.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
        )
        print(f"âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {self.collection_name}")
    
    def chunk_text(self, text, chunk_size=200, overlap=50):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ìµœì í™”: 200 words, 50 overlap = 25%)"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if len(chunk.strip()) > 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸ (100â†’50ìœ¼ë¡œ ì™„í™”)
                chunks.append(chunk)

        return chunks
    
    def create_embeddings(self, extracted_json_path):
        """JSONì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ì„ë² ë”© ìƒì„±"""
        print(f"\nğŸ“š í…ìŠ¤íŠ¸ ë¡œë“œ: {extracted_json_path}")
        
        with open(extracted_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ì´ {data['total_pages']}í˜ì´ì§€")
        
        # ì²­í‚¹
        all_chunks = []
        for page in data['pages']:
            chunks = self.chunk_text(page['text'])
            for chunk_idx, chunk in enumerate(chunks):
                all_chunks.append({
                    "content": chunk,
                    "page": page['page_number'],
                    "chunk_idx": chunk_idx
                })
        
        print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(all_chunks)}")
        
        # ì„ë² ë”© ë° ì €ì¥
        points = []
        print("\nğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        for idx, chunk_data in enumerate(tqdm(all_chunks)):
            # ì„ë² ë”©
            vector = self.embedder.encode(chunk_data['content']).tolist()
            
            # í¬ì¸íŠ¸ ìƒì„±
            point = PointStruct(
                id=idx,
                vector=vector,
                payload={
                    "content": chunk_data['content'],
                    "page": chunk_data['page'],
                    "chunk_idx": chunk_data['chunk_idx'],
                    "source": "uploaded_report.pdf"
                }
            )
            points.append(point)
        
        # Qdrantì— ì €ì¥
        print(f"\nğŸ’¾ Qdrantì— ì €ì¥ ì¤‘...")
        self.qdrant.upsert(
            collection_name=self.collection_name,
            points=points
        )
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {len(points)}ê°œ ì²­í¬")
        
        return len(points)

def main():
    # ê²½ë¡œ ì„¤ì •
    extracted_json = Path('data/extracted_text.json')
    
    if not extracted_json.exists():
        print(f"âŒ {extracted_json}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € 1_parse_pdf.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # ë²¡í„° DB ë¹Œë” ì´ˆê¸°í™”
    builder = VectorDBBuilder()
    
    # ì»¬ë ‰ì…˜ ìƒì„±
    builder.init_collection()
    
    # ì„ë² ë”© ìƒì„± ë° ì €ì¥
    chunk_count = builder.create_embeddings(extracted_json)
    
    print(f"\nâœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ!")
    print(f"   - ì´ ì²­í¬ ìˆ˜: {chunk_count}")
    print(f"   - ì»¬ë ‰ì…˜: {builder.collection_name}")

if __name__ == "__main__":
    main()
